{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl8quffcowKu"
      },
      "source": [
        "# Instance Segmenation based on YOLO11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.\n",
        "\n",
        "[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://www.pepy.tech/projects/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/healthonrails/annolid/blob/main/docs/tutorials/Annolid_Instance_Segmentation_on_YOLO11_Tutorial.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbvMlHd_QwMG"
      },
      "outputs": [],
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJwI06VXAd5l"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcCJTU46pMvX"
      },
      "source": [
        "# Upload custom dataset, labeled in annolid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um5pNZOIoaQU"
      },
      "outputs": [],
      "source": [
        "custom_dataset = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaxatJbaFs7Z"
      },
      "outputs": [],
      "source": [
        "!unzip YOLO_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy5O6ksWC02L"
      },
      "outputs": [],
      "source": [
        "# Path to the dataset directory\n",
        "image_dir = '/content/YOLO_dataset/images/val'  # Directory containing images\n",
        "label_dir = '/content/YOLO_dataset/labels/val'  # Directory containing YOLO Pose estimation labels (e.g., .txt files)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e14h2gAfpVi3"
      },
      "source": [
        "# Optional (check dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ8XV9xPCvQE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_pose_estimation_with_labels(image_path, label_path, keypoint_color=(0, 0, 255), keypoint_radius=5):\n",
        "    \"\"\"\n",
        "    Visualizes pose estimation keypoints and bounding boxes from YOLO format label files on an image.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file.\n",
        "        label_path (str): Path to the YOLO format label file.\n",
        "        keypoint_color (tuple): BGR color for keypoints (default: blue).\n",
        "        keypoint_radius (int): Radius of the circles drawn for keypoints (default: 5).\n",
        "    \"\"\"\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    if not os.path.exists(label_path):\n",
        "        print(f\"Label file {label_path} not found.\")\n",
        "        return\n",
        "\n",
        "    with open(label_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        data = line.strip().split()\n",
        "        class_id = int(data[0])\n",
        "\n",
        "        # Extract bounding box coordinates (x_center, y_center, width, height)\n",
        "        center_x = float(data[1])\n",
        "        center_y = float(data[2])\n",
        "        box_width = float(data[3])\n",
        "        box_height = float(data[4])\n",
        "\n",
        "        # Denormalize bounding box coordinates\n",
        "        x_min = int((center_x - box_width / 2) * img.shape[1])\n",
        "        y_min = int((center_y - box_height / 2) * img.shape[0])\n",
        "        x_max = int((center_x + box_width / 2) * img.shape[1])\n",
        "        y_max = int((center_y + box_height / 2) * img.shape[0])\n",
        "\n",
        "        # Extract keypoint coordinates (px1, py1, px2, py2, ...)\n",
        "        keypoints_data = data[5:]\n",
        "        num_keypoints = len(keypoints_data) // 2\n",
        "        keypoints = []\n",
        "        for i in range(num_keypoints):\n",
        "            px = float(keypoints_data[2*i])\n",
        "            py = float(keypoints_data[2*i + 1])\n",
        "            keypoints.append((px, py))\n",
        "\n",
        "        # Visualize bounding box (optional, you can comment this out if you only want keypoints)\n",
        "        cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=1)\n",
        "\n",
        "        # Visualize keypoints\n",
        "        for px, py in keypoints:\n",
        "            # Denormalize keypoint coordinates\n",
        "            keypoint_x = int(px * img.shape[1])\n",
        "            keypoint_y = int(py * img.shape[0])\n",
        "            cv2.circle(img, (keypoint_x, keypoint_y), keypoint_radius, keypoint_color, -1) # Draw filled circle\n",
        "\n",
        "        # Display Class ID near the bounding box (you can adjust position)\n",
        "        cv2.putText(img, f'Class {class_id}', (x_min, y_min - 10 if y_min > 20 else y_min + 20), # Adjust text position\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA) # Reduced thickness for clarity\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Pose Estimation with Ground Truth: {os.path.basename(image_path)}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show() # Added plt.show() to display each image in loop\n",
        "\n",
        "if not os.path.exists(image_dir):\n",
        "    print(f\"Error: Image directory '{image_dir}' not found.\")\n",
        "elif not os.path.exists(label_dir):\n",
        "    print(f\"Error: Label directory '{label_dir}' not found.\")\n",
        "else:\n",
        "    # Loop through images and visualize them with pose estimation labels\n",
        "    for image_name in os.listdir(image_dir):\n",
        "        if image_name.endswith(('.jpg', '.jpeg', '.png')): # Added common image extensions\n",
        "            image_path = os.path.join(image_dir, image_name)\n",
        "            label_path = os.path.join(label_dir, os.path.splitext(image_name)[0] + '.txt')\n",
        "            visualize_pose_estimation_with_labels(image_path, label_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# Predict without fine-tuning\n",
        "\n",
        "YOLO11 may be used directly in the Command Line Interface (CLI) with a `yolo` command for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See a full list of available `yolo` [arguments](https://docs.ultralytics.com/usage/cfg/) and other details in the [YOLO11 Predict Docs](https://docs.ultralytics.com/modes/train/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR9ZbuQCH7FX"
      },
      "outputs": [],
      "source": [
        "# Run inference on an image with YOLO11n\n",
        "!yolo predict model=yolo11n-pose.pt source='/content/YOLO_dataset/images/val/92-mouse-2_000000000.png'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkAzDWJ7cWTr"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktegpM42AooT"
      },
      "outputs": [],
      "source": [
        "#@title Select YOLO11 üöÄ logger {run: 'auto'}\n",
        "logger = 'TensorBoard' #@param ['Comet', 'TensorBoard']\n",
        "\n",
        "if logger == 'Comet':\n",
        "  %pip install -q comet_ml\n",
        "  import comet_ml; comet_ml.init()\n",
        "elif logger == 'TensorBoard':\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NcFxRcFdJ_O"
      },
      "outputs": [],
      "source": [
        "# Train YOLO11n-seg on custom dataset for 30 epochs\n",
        "!yolo train model=yolo11n-pose.pt data=/content/YOLO_dataset/data.yaml epochs=300 imgsz=640"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZm1AsHDpgZ9"
      },
      "source": [
        "# Inference and Save results to annolid json files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmH0bbc9Rsz7"
      },
      "outputs": [],
      "source": [
        "# Run inference on an image with YOLO11n\n",
        "!yolo predict model=runs/pose/train/weights/best.pt source='/content/YOLO_dataset/images/val/92-mouse-2_000000000.png'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EnE1-PXFBvM6"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the YOLO11n pose estimation model\n",
        "#model = YOLO(\"yolo11n-pose.pt\")  # Load a pretrained YOLO11 pose model, you can replace with your custom model path\n",
        "model = YOLO(\"runs/pose/train2/weights/best.pt\")  # Update this to your trained pose model if you have one\n",
        "\n",
        "# Provide the path to your video file\n",
        "video_path = \"/content/92-mouse-2.mp4\"  # Update this to your video file's path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Create an output directory named after the video file without the extension\n",
        "video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "output_dir = video_name + \"_pose_estimation\" # Updated output directory name to reflect pose estimation\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Store the track history\n",
        "track_history = defaultdict(lambda: [])\n",
        "frame_id = 0  # Track frame number\n",
        "display_interval = 10  # Display visualization every n frames\n",
        "\n",
        "# Loop through the video frames\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if success:\n",
        "        # Run YOLOv8 pose estimation tracking on the frame, persisting tracks between frames\n",
        "        results = model.track(frame, persist=True)\n",
        "\n",
        "        # Initialize LabelMe JSON structure\n",
        "        labelme_data = {\n",
        "            \"version\": \"5.5.0\", # Updated version to match current LabelMe version\n",
        "            \"flags\": {},\n",
        "            \"shapes\": [],\n",
        "            \"imagePath\": os.path.basename(video_path), # Added imagePath to be more informative\n",
        "            \"imageHeight\": frame.shape[0],\n",
        "            \"imageWidth\": frame.shape[1],\n",
        "            \"imageData\": None, # set to None as imageData is usually not needed for video annotation\n",
        "        }\n",
        "\n",
        "        # Get the boxes, track IDs, and pose keypoints\n",
        "        boxes = results[0].boxes.xywh.cpu()\n",
        "        try:\n",
        "          track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "        except:\n",
        "          track_ids = [0]\n",
        "        keypoints_list = results[0].keypoints.xy.cpu().numpy() # Get keypoints\n",
        "\n",
        "        # Loop through detected objects\n",
        "        for box, track_id, keypoints in zip(boxes, track_ids, keypoints_list): # Looping through keypoints as well\n",
        "            x, y, w, h = box.tolist()\n",
        "            track = track_history[track_id]\n",
        "            track.append((float(x), float(y)))  # x, y center point\n",
        "\n",
        "            if len(track) > 30:  # retain 30 tracks for 30 frames\n",
        "                track.pop(0)\n",
        "\n",
        "            # Bounding box coordinates\n",
        "            x1, y1 = x - w/2, y - h/2\n",
        "            x2, y2 = x + w/2, y + h/2\n",
        "\n",
        "            # Add bounding box annotation\n",
        "            bbox_shape = {\n",
        "                \"label\": f\"object_{track_id}\",\n",
        "                \"shape_type\": \"rectangle\",\n",
        "                \"points\": [\n",
        "                    [float(x1), float(y1)],\n",
        "                    [float(x2), float(y2)]\n",
        "                ],\n",
        "                \"group_id\": track_id,\n",
        "                \"description\": \"Bounding Box\", # Added description for clarity\n",
        "                \"flags\": {},\n",
        "                \"line_color\": None,\n",
        "                \"fill_color\": None\n",
        "            }\n",
        "            labelme_data[\"shapes\"].append(bbox_shape)\n",
        "\n",
        "            # Add polygon for tracking history\n",
        "            if len(track) > 1:\n",
        "                points = np.array(track).tolist()\n",
        "                shape_polygon = {\n",
        "                    \"label\": f\"track_{track_id}\",\n",
        "                    \"shape_type\": \"polygon\",\n",
        "                    \"points\": points,\n",
        "                    \"group_id\": track_id,\n",
        "                    \"description\": \"Tracking History\", # Added description for clarity\n",
        "                    \"flags\": {},\n",
        "                    \"line_color\": None,\n",
        "                    \"fill_color\": None\n",
        "                }\n",
        "                labelme_data[\"shapes\"].append(shape_polygon)\n",
        "\n",
        "            # Add keypoint annotations\n",
        "            for idx, (kx, ky) in enumerate(keypoints):\n",
        "                keypoint_shape = {\n",
        "                    \"label\": f\"keypoint_{track_id}_{idx}\", # Label each keypoint with track ID and index\n",
        "                    \"shape_type\": \"point\",\n",
        "                    \"points\": [[float(kx), float(ky)]],\n",
        "                    \"group_id\": track_id,\n",
        "                    \"description\": f\"Keypoint {idx}\", # Added description for clarity\n",
        "                    \"flags\": {},\n",
        "                    \"line_color\": None,\n",
        "                    \"fill_color\": None\n",
        "                }\n",
        "                labelme_data[\"shapes\"].append(keypoint_shape)\n",
        "\n",
        "\n",
        "        # Save the JSON annotation file with zero-padded numbering (e.g., 000000001.json)\n",
        "        json_filename = os.path.join(output_dir, f\"{frame_id:09d}.json\")\n",
        "        with open(json_filename, \"w\") as json_file:\n",
        "            json.dump(labelme_data, json_file, indent=4)\n",
        "\n",
        "        # Visualization: Display annotated frame every 'display_interval' frames\n",
        "        if frame_id % display_interval == 0:\n",
        "            annotated_frame = results[0].plot()  # Get annotated frame with pose estimation\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.axis('off')\n",
        "            plt.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
        "            clear_output(wait=True)  # Clear previous output for smoother display\n",
        "            plt.show()\n",
        "\n",
        "        frame_id += 1  # Increment frame number\n",
        "    else:\n",
        "        # Break the loop if the end of the video is reached\n",
        "        break\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()\n",
        "print(f\"Pose estimation annotations saved in '{output_dir}' directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoVDypQ0px11"
      },
      "source": [
        "# Zip and download json results can be loaded into annolid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-OfoVvzWA4K"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Provide the path to the folder you want to zip\n",
        "folder_to_zip = output_dir  # Update with your folder path\n",
        "\n",
        "# Output zip file path\n",
        "output_zip_file = folder_to_zip + \".zip\"\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(folder_to_zip, 'zip', folder_to_zip)\n",
        "\n",
        "print(f\"Folder '{folder_to_zip}' has been zipped to '{output_zip_file}'.\")\n",
        "files.download(output_zip_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUMFYuLk_waJ"
      },
      "source": [
        "# Zip and download runs folder which continas the saved best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuAW2iF8_jbq"
      },
      "outputs": [],
      "source": [
        "# Replace 'folder_name' with the name of your folder\n",
        "folder_to_download = 'runs'\n",
        "output_filename = 'runs.zip'\n",
        "\n",
        "# Compress the folder\n",
        "shutil.make_archive(output_filename.replace('.zip', ''), 'zip', folder_to_download)\n",
        "\n",
        "# Download the zipped folder\n",
        "files.download(output_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPZZeNrLCQG6"
      },
      "source": [
        "# 4. Export\n",
        "\n",
        "Export a YOLO11 model to any supported format below with the `format` argument, i.e. `format=onnx`. See [YOLO11 Export Docs](https://docs.ultralytics.com/modes/export/) for more information.\n",
        "\n",
        "- üí° ProTip: Export to [ONNX](https://docs.ultralytics.com/integrations/onnx/) or [OpenVINO](https://docs.ultralytics.com/integrations/openvino/) for up to 3x CPU speedup.  \n",
        "- üí° ProTip: Export to [TensorRT](https://docs.ultralytics.com/integrations/tensorrt/) for up to 5x GPU speedup.\n",
        "\n",
        "| Format                                                                   | `format` Argument | Model                     | Metadata | Arguments                                                            |\n",
        "|--------------------------------------------------------------------------|-------------------|---------------------------|----------|----------------------------------------------------------------------|\n",
        "| [PyTorch](https://pytorch.org/)                                          | -                 | `yolo11n.pt`              | ‚úÖ        | -                                                                    |\n",
        "| [TorchScript](https://docs.ultralytics.com/integrations/torchscript)     | `torchscript`     | `yolo11n.torchscript`     | ‚úÖ        | `imgsz`, `optimize`, `batch`                                         |\n",
        "| [ONNX](https://docs.ultralytics.com/integrations/onnx)                   | `onnx`            | `yolo11n.onnx`            | ‚úÖ        | `imgsz`, `half`, `dynamic`, `simplify`, `opset`, `batch`             |\n",
        "| [OpenVINO](https://docs.ultralytics.com/integrations/openvino)           | `openvino`        | `yolo11n_openvino_model/` | ‚úÖ        | `imgsz`, `half`, `int8`, `batch`                                     |\n",
        "| [TensorRT](https://docs.ultralytics.com/integrations/tensorrt)           | `engine`          | `yolo11n.engine`          | ‚úÖ        | `imgsz`, `half`, `dynamic`, `simplify`, `workspace`, `int8`, `batch` |\n",
        "| [CoreML](https://docs.ultralytics.com/integrations/coreml)               | `coreml`          | `yolo11n.mlpackage`       | ‚úÖ        | `imgsz`, `half`, `int8`, `nms`, `batch`                              |\n",
        "| [TF SavedModel](https://docs.ultralytics.com/integrations/tf-savedmodel) | `saved_model`     | `yolo11n_saved_model/`    | ‚úÖ        | `imgsz`, `keras`, `int8`, `batch`                                    |\n",
        "| [TF GraphDef](https://docs.ultralytics.com/integrations/tf-graphdef)     | `pb`              | `yolo11n.pb`              | ‚ùå        | `imgsz`, `batch`                                                     |\n",
        "| [TF Lite](https://docs.ultralytics.com/integrations/tflite)              | `tflite`          | `yolo11n.tflite`          | ‚úÖ        | `imgsz`, `half`, `int8`, `batch`                                     |\n",
        "| [TF Edge TPU](https://docs.ultralytics.com/integrations/edge-tpu)        | `edgetpu`         | `yolo11n_edgetpu.tflite`  | ‚úÖ        | `imgsz`                                                              |\n",
        "| [TF.js](https://docs.ultralytics.com/integrations/tfjs)                  | `tfjs`            | `yolo11n_web_model/`      | ‚úÖ        | `imgsz`, `half`, `int8`, `batch`                                     |\n",
        "| [PaddlePaddle](https://docs.ultralytics.com/integrations/paddlepaddle)   | `paddle`          | `yolo11n_paddle_model/`   | ‚úÖ        | `imgsz`, `batch`                                                     |\n",
        "| [NCNN](https://docs.ultralytics.com/integrations/ncnn)                   | `ncnn`            | `yolo11n_ncnn_model/`     | ‚úÖ        | `imgsz`, `half`, `batch`                                             |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYIjW4igCjqD"
      },
      "outputs": [],
      "source": [
        "!yolo export model=runs/pose/train/weights/best.pt format=onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUMOQ0OeDBJG"
      },
      "source": [
        "# 5. Python Usage\n",
        "\n",
        "YOLO11 was reimagined using Python-first principles for the most seamless Python YOLO experience yet. YOLO11 models can be loaded from a trained checkpoint or created from scratch. Then methods are used to train, val, predict, and export the model. See detailed Python usage examples in the [YOLO11 Python Docs](https://docs.ultralytics.com/usage/python/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpF9-vS_DAaf"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO('runs/pose/train/weights/best.pt')  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Use the model\n",
        "results = model('/content/YOLO_dataset/images/val/92-mouse-2_000000000.png')  # predict on an image\n",
        "results = model.export(format='onnx')  # export the model to ONNX formats"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}