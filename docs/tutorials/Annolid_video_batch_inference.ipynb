{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Annolid_video_batch_inference",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/healthonrails/annolid/blob/main/docs/tutorials/Annolid_video_batch_inference.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "gtCYnCrB8BxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This* notebook provides recipes for batch inference on your videos with a trained model and the datasets used for training the model.\n",
        "You need to upload your the following files from your local drive or mount the Google Drive that contains them. \n",
        "1. Custom COCO Format Dataset (e.g. my_coco_dataset.zip)\n",
        "2. A trained model saved as .pth format (e.g. model_final.pth)\n",
        "3. A folder contains all the videos (e.g. my_videos folder has video files like 1.mp4, 2.avi ...)"
      ],
      "metadata": {
        "id": "7Z2jcRKwUHqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Detectron2"
      ],
      "metadata": {
        "id": "7he396rSMrBX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# install dependencies: \n",
        "!pip install pyyaml==5.3\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab"
      ],
      "outputs": [],
      "metadata": {
        "id": "vlSWv54MMpNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## It takes a few mintures to install Detectron2 from source. "
      ],
      "metadata": {
        "id": "3EIIwBeZYuhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "metadata": {
        "id": "dCgNy10xYbf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and install Annolid"
      ],
      "metadata": {
        "id": "Yf7YXWfFZsmf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!git clone --recurse-submodules https://github.com/healthonrails/annolid.git"
      ],
      "outputs": [],
      "metadata": {
        "id": "3HnNyOswKbC5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%cd annolid\n",
        "!pip install -e .\n",
        "%cd /content"
      ],
      "outputs": [],
      "metadata": {
        "id": "WlKFjlxuLRA3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import requests\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from annolid.annotation.keypoints import save_labels\n",
        "from annolid.postprocessing.quality_control import pred_dict_to_labelme\n",
        "from annolid.data.videos import frame_from_video\n",
        "from annolid.inference.predict import Segmentor"
      ],
      "outputs": [],
      "metadata": {
        "id": "wsW5Q6dwcrta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Local file system (Please skip this section if you want to use the files in your Google Drive)"
      ],
      "metadata": {
        "id": "eikfzi8ZT_rW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading files from your local file system\n",
        "\n",
        "`files.upload` returns a dictionary of the files which were uploaded.\n",
        "The dictionary is keyed by the file name and values are the data which were uploaded."
      ],
      "metadata": {
        "id": "BaCkyg5CV5jF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "outputs": [],
      "metadata": {
        "id": "vz-jH8T_Uk2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Or mount your Google Drive on this runtime and accesss files from there"
      ],
      "metadata": {
        "id": "64TzYFFufN8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "id": "dhVnij13fm1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example for unzip your COCO format dataset zip file"
      ],
      "metadata": {
        "id": "VrhkLihIiKpt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!unzip /content/digging_videos_coco_dataset.zip -d /content"
      ],
      "outputs": [],
      "metadata": {
        "id": "ymtCoovZM85D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "DATASET_DIR = \"/content/my_videos_coco_dataset\"\n",
        "MODEL_PATH =  \"/content/model_final.pth\"\n",
        "VIDEOS_FOLDER = '/content/my_videos'"
      ],
      "outputs": [],
      "metadata": {
        "id": "UqPPW1kMM-AH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the predictor"
      ],
      "metadata": {
        "id": "hPPf4-n7aLMF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "predictor = Segmentor(DATASET_DIR,MODEL_PATH)"
      ],
      "outputs": [],
      "metadata": {
        "id": "RWJ0BeYdOpdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find all the video files in the given `VIDEOS_FOLDER`"
      ],
      "metadata": {
        "id": "O5b7pb7oljyO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "video_files = glob.glob(VIDEOS_FOLDER + '/*.*')"
      ],
      "outputs": [],
      "metadata": {
        "id": "frXbD7n6ZG5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch inference for the videos"
      ],
      "metadata": {
        "id": "zxe7zEbZl1nS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# select number of frames to skip e.g. 2 for every other frames \n",
        "# if skip_frames = 30, every 30th frame will be processed.\n",
        "# default 1 no skipping\n",
        "skip_frames = 1 #@param {type: \"slider\", min:0, max:30, step: 1}\n",
        "for video_file in video_files:\n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "  if cap.isOpened():\n",
        "    cap.release()\n",
        "    print(\"Working on video, \", video_file)\n",
        "    predictor.on_video(video_file,skip_frames=skip_frames)\n",
        "  else:\n",
        "    print(\"Cannot open this file\", video_file)\n",
        "    cap.release()"
      ],
      "outputs": [],
      "metadata": {
        "id": "UGe0hazbahlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading files to your local file system\n",
        "\n",
        "`files.download` will invoke a browser download of the file to your local computer.\n"
      ],
      "metadata": {
        "id": "hauvGV4hV-Mh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import files\n",
        "tracking_csv_files = glob.glob(str(Path(DATASET_DIR).parent) + '/*mask*tracking*.csv')\n",
        "for tcf in tracking_csv_files:\n",
        "    files.download(tcf)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0188d904-7c60-4884-a602-8fbf9660b772\", \"digging_videos_coco_dataset_video_top3_mask_rcnn_tracking_results_with_segmentation.csv\", 98255)"
            ]
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "id": "p2E4EKhCWEC5",
        "outputId": "ecad04bc-f45b-4702-d6b6-273c75ce3108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Or saving the tracking results csv files to your Google Drive\n",
        "\n"
      ],
      "metadata": {
        "id": "u22w3BFiOveA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import shutil"
      ],
      "outputs": [],
      "metadata": {
        "id": "RWSJpsyKqHjH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for tcf in tracking_csv_files:\n",
        "    shutil.copy(tcf, '/content/drive/MyDrive/')"
      ],
      "outputs": [],
      "metadata": {
        "id": "tYXQViLfAvwk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "outputs": [],
      "metadata": {
        "id": "D78AM1fFt2ty"
      }
    }
  ]
}