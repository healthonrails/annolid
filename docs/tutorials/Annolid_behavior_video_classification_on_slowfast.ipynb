{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2187f5",
   "metadata": {
    "id": "8a2187f5"
   },
   "source": [
    "# Annolid Behavior Video Classification on SlowFast Tutorial\n",
    "\n",
    "This Colab contains a tutorial on how to perform behavior video classification using the Annolid library and a SlowFast model.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/healthonrails/annolid/blob/main/docs/tutorials/Annolid_behavior_video_classification_on_slowfast.ipynb)\n",
    "# SlowFast\n",
    "\n",
    "*Author: FAIR PyTorchVideo*\n",
    "\n",
    "Modified from: https://pytorch.org/hub/facebookresearch_pytorchvideo_slowfast/\n",
    "\n",
    "**SlowFast networks pretrained fine-tuning on the custom behavior dataset**\n",
    "\n",
    "#### Install required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_nnS3TXkm0uH",
   "metadata": {
    "id": "_nnS3TXkm0uH"
   },
   "outputs": [],
   "source": [
    "!pip install fvcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qev6RUwGnAiZ",
   "metadata": {
    "id": "qev6RUwGnAiZ"
   },
   "outputs": [],
   "source": [
    "!pip install pyav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84597b6a",
   "metadata": {
    "id": "84597b6a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.hub import load\n",
    "\n",
    "# Load the pre-trained SlowFast model\n",
    "model = load(\"facebookresearch/pytorchvideo\", \"slowfast_r50\", pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a61a8c",
   "metadata": {
    "id": "37a61a8c"
   },
   "source": [
    "Import remaining functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d46ea",
   "metadata": {
    "id": "e79d46ea"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef056027",
   "metadata": {
    "id": "ef056027"
   },
   "source": [
    "#### Setup\n",
    "\n",
    "Set the model to eval mode and move to desired device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703119e4",
   "metadata": {
    "attributes": {
     "classes": [
      "python "
     ],
     "id": ""
    },
    "id": "703119e4"
   },
   "outputs": [],
   "source": [
    "# Set to GPU or CPU\n",
    "device = \"cuda\"\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19FWRf2ZohDW",
   "metadata": {
    "id": "19FWRf2ZohDW"
   },
   "outputs": [],
   "source": [
    "!gdown --id 15fnvK0KS9rQdoAB1yK2kw5WFVws_BlW5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nEGQ6k7ooocy",
   "metadata": {
    "id": "nEGQ6k7ooocy"
   },
   "outputs": [],
   "source": [
    "!unzip behavior_videos.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HAH_Oyzfo30Y",
   "metadata": {
    "id": "HAH_Oyzfo30Y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "video_annotation = \"behaivor_videos/amygdala control/CaMKII 1-19 L 2-19-21-Phase 3.csv\"\n",
    "df_anno = pd.read_csv(video_annotation)\n",
    "df_anno[df_anno.Behavior == \"Grooming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-vnSALEDo9Iv",
   "metadata": {
    "id": "-vnSALEDo9Iv"
   },
   "outputs": [],
   "source": [
    "behaviors = df_anno.Behavior.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bahD7b61o-lt",
   "metadata": {
    "id": "bahD7b61o-lt"
   },
   "outputs": [],
   "source": [
    "behaviors = behaviors + [\n",
    "    \"FP Mouse Mounting Stimulus Mouse\",\n",
    "    \"FP Mouse Snigging Excretions\",\n",
    "    \"FP Mouse Tail Rattling\",\n",
    "    \"Others\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DwGJ0dygpByp",
   "metadata": {
    "id": "DwGJ0dygpByp"
   },
   "outputs": [],
   "source": [
    "behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tO3JwcGYq7Cf",
   "metadata": {
    "id": "tO3JwcGYq7Cf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# Constants for file paths and split ratio\n",
    "BASE_FOLDER = \"behaivor_videos\"\n",
    "OUTPUT_VIDEO_FOLDER = \"behavior_video_clips\"\n",
    "TRAIN_JSONL_PATH = \"train_video_annotations.jsonl\"\n",
    "TEST_JSONL_PATH = \"test_video_annotations.jsonl\"\n",
    "TRAIN_SPLIT_RATIO = 0.97\n",
    "\n",
    "# Parameters for SlowFast data preparation\n",
    "SLOW_FPS_REDUCTION_FACTOR = 4  # Factor by which slow pathway FPS is reduced\n",
    "TARGET_FPS = 30  # Assuming original video FPS is around 30, adjust if needed\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(OUTPUT_VIDEO_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_video_segment(video_file_path, start_time, end_time, output_path):\n",
    "    \"\"\"Extracts a video segment from a file between start and end times.\"\"\"\n",
    "    ffmpeg_extract_subclip(\n",
    "        video_file_path, start_time, end_time, targetname=output_path\n",
    "    )\n",
    "\n",
    "\n",
    "def create_annotation_entry(\n",
    "    behavior,\n",
    "    video_segment_path_slow,\n",
    "    video_segment_path_fast,\n",
    "    prompt=\"<video> What is the behavior in the video?\",\n",
    "):\n",
    "    \"\"\"Creates a JSONL entry for a video segment, storing paths for both slow and fast pathways.\"\"\"\n",
    "    return {\n",
    "        \"query\": prompt,\n",
    "        \"response\": behavior,\n",
    "        \"videos\": [video_segment_path_slow, video_segment_path_fast],\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_behavior_events(csv_path):\n",
    "    \"\"\"Parses start and stop events from a behavior CSV file.\"\"\"\n",
    "    start_events, stop_events = [], []\n",
    "    with open(csv_path, \"r\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            time = float(row[\"Recording time\"])\n",
    "            behavior = row[\"Behavior\"]\n",
    "            event = row[\"Event\"]\n",
    "            if event == \"state start\":\n",
    "                start_events.append({\"time\": time, \"behavior\": behavior})\n",
    "            elif event == \"state stop\":\n",
    "                stop_events.append({\"time\": time, \"behavior\": behavior})\n",
    "    return start_events, stop_events\n",
    "\n",
    "\n",
    "def find_gaps(start_events, stop_events, video_duration, gap_duration):\n",
    "    \"\"\"Finds gaps between behavior events to sample as 'Others'.\"\"\"\n",
    "    gaps = []\n",
    "    last_end_time = 0\n",
    "\n",
    "    for start_event in start_events:\n",
    "        start_time = start_event[\"time\"]\n",
    "        if start_time - last_end_time >= gap_duration:\n",
    "            gaps.append((last_end_time, start_time))\n",
    "\n",
    "        matching_stop = next(\n",
    "            (\n",
    "                s\n",
    "                for s in stop_events\n",
    "                if s[\"behavior\"] == start_event[\"behavior\"] and s[\"time\"] > start_time\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        if matching_stop:\n",
    "            last_end_time = matching_stop[\"time\"]\n",
    "            stop_events.remove(matching_stop)\n",
    "\n",
    "    if video_duration - last_end_time >= gap_duration:\n",
    "        gaps.append((last_end_time, video_duration))\n",
    "\n",
    "    return gaps\n",
    "\n",
    "\n",
    "def sample_limited_segments_from_gaps(\n",
    "    gaps, video_file_path, video_name, gap_duration, max_count, behavior_label=\"Others\"\n",
    "):\n",
    "    \"\"\"Samples segments from the gaps with a limit on the number of segments for 'Others' behavior, creating slow and fast versions.\"\"\"\n",
    "    entries = []\n",
    "    for start, end in gaps:\n",
    "        num_segments = int((end - start) // gap_duration)\n",
    "        for i in range(min(num_segments, max_count - len(entries))):\n",
    "            segment_start = start + i * gap_duration\n",
    "            segment_end = segment_start + gap_duration\n",
    "\n",
    "            # Create paths for slow and fast segments (both are initially the same temporal segment)\n",
    "            segment_path_slow = f\"{OUTPUT_VIDEO_FOLDER}/{video_name}_other_slow_{segment_start}-{segment_end}.mp4\"\n",
    "            segment_path_fast = f\"{OUTPUT_VIDEO_FOLDER}/{video_name}_other_fast_{segment_start}-{segment_end}.mp4\"\n",
    "\n",
    "            # Extract both segments\n",
    "            extract_video_segment(\n",
    "                video_file_path, segment_start, segment_end, segment_path_slow\n",
    "            )\n",
    "            extract_video_segment(\n",
    "                video_file_path, segment_start, segment_end, segment_path_fast\n",
    "            )\n",
    "\n",
    "            entries.append(\n",
    "                create_annotation_entry(\n",
    "                    behavior_label, segment_path_slow, segment_path_fast\n",
    "                )\n",
    "            )\n",
    "            if len(entries) >= max_count:\n",
    "                break\n",
    "        if len(entries) >= max_count:\n",
    "            break\n",
    "    return entries\n",
    "\n",
    "\n",
    "def process_video_file(csv_path, video_file_path, gap_duration=5):\n",
    "    \"\"\"Processes a video file by extracting labeled and limited 'Others' segments, creating slow and fast versions.\"\"\"\n",
    "    start_events, stop_events = parse_behavior_events(csv_path)\n",
    "    video_name = os.path.splitext(os.path.basename(video_file_path))[0].replace(\n",
    "        \" \", \"_\"\n",
    "    )\n",
    "    labeled_entries = []\n",
    "    behavior_counts = defaultdict(int)\n",
    "\n",
    "    for start_event in start_events:\n",
    "        start_time = start_event[\"time\"]\n",
    "        behavior = start_event[\"behavior\"]\n",
    "        matching_stop = next(\n",
    "            (\n",
    "                stop\n",
    "                for stop in stop_events\n",
    "                if stop[\"behavior\"] == behavior and stop[\"time\"] > start_time\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        if matching_stop:\n",
    "            end_time = matching_stop[\"time\"]\n",
    "            stop_events.remove(matching_stop)\n",
    "\n",
    "            # Create paths for slow and fast segments\n",
    "            segment_path_slow = f\"{OUTPUT_VIDEO_FOLDER}/{video_name}_{behavior.replace(' ', '_')}_slow_{start_time}-{end_time}.mp4\"\n",
    "            segment_path_fast = f\"{OUTPUT_VIDEO_FOLDER}/{video_name}_{behavior.replace(' ', '_')}_fast_{start_time}-{end_time}.mp4\"\n",
    "\n",
    "            # Extract both segments\n",
    "            extract_video_segment(\n",
    "                video_file_path, start_time, end_time, segment_path_slow\n",
    "            )\n",
    "            extract_video_segment(\n",
    "                video_file_path, start_time, end_time, segment_path_fast\n",
    "            )\n",
    "\n",
    "            labeled_entries.append(\n",
    "                create_annotation_entry(behavior, segment_path_slow, segment_path_fast)\n",
    "            )\n",
    "            behavior_counts[behavior] += 1\n",
    "\n",
    "    max_behavior_count = max(behavior_counts.values(), default=0)\n",
    "\n",
    "    with VideoFileClip(video_file_path) as video:\n",
    "        video_duration = video.duration\n",
    "    gaps = find_gaps(start_events, stop_events, video_duration, gap_duration)\n",
    "    other_entries = sample_limited_segments_from_gaps(\n",
    "        gaps, video_file_path, video_name, gap_duration, max_behavior_count\n",
    "    )\n",
    "\n",
    "    return labeled_entries + other_entries\n",
    "\n",
    "\n",
    "def stratified_interleaved_split_and_save_annotations(\n",
    "    entries, train_path, test_path, train_ratio=0.97\n",
    "):\n",
    "    \"\"\"Splits annotations into stratified, interleaved train and test sets by behavior and saves them to JSONL files.\"\"\"\n",
    "    behavior_groups = defaultdict(list)\n",
    "    for entry in entries:\n",
    "        behavior = entry[\"response\"]\n",
    "        behavior_groups[behavior].append(entry)\n",
    "\n",
    "    train_entries, test_entries = [], []\n",
    "    for behavior, group_entries in behavior_groups.items():\n",
    "        random.shuffle(group_entries)\n",
    "        split_index = int(len(group_entries) * train_ratio)\n",
    "        train_entries.append(group_entries[:split_index])\n",
    "        test_entries.append(group_entries[split_index:])\n",
    "\n",
    "    interleaved_train = list(\n",
    "        itertools.chain.from_iterable(itertools.zip_longest(*train_entries))\n",
    "    )\n",
    "    interleaved_test = list(\n",
    "        itertools.chain.from_iterable(itertools.zip_longest(*test_entries))\n",
    "    )\n",
    "\n",
    "    interleaved_train = [entry for entry in interleaved_train if entry is not None]\n",
    "    interleaved_test = [entry for entry in interleaved_test if entry is not None]\n",
    "\n",
    "    with open(train_path, \"w\") as f:\n",
    "        for entry in interleaved_train:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "    with open(test_path, \"w\") as f:\n",
    "        for entry in interleaved_test:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "\n",
    "def process_dataset():\n",
    "    \"\"\"Processes the entire dataset and creates stratified interleaved train/test JSONL files for SlowFast.\"\"\"\n",
    "    all_entries = []\n",
    "    for subdir in os.listdir(BASE_FOLDER):\n",
    "        subdir_path = os.path.join(BASE_FOLDER, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for file in os.listdir(subdir_path):\n",
    "                if file.endswith(\".csv\"):\n",
    "                    csv_path = os.path.join(subdir_path, file)\n",
    "                    video_file_path = csv_path.replace(\".csv\", \".mpg\")\n",
    "                    if os.path.exists(video_file_path):\n",
    "                        entries = process_video_file(csv_path, video_file_path)\n",
    "                        all_entries.extend(entries)\n",
    "\n",
    "    stratified_interleaved_split_and_save_annotations(\n",
    "        all_entries, TRAIN_JSONL_PATH, TEST_JSONL_PATH, TRAIN_SPLIT_RATIO\n",
    "    )\n",
    "    print(\n",
    "        \"Conversion complete. Stratified interleaved training and testing datasets created for SlowFast.\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8oIM0e409Yi",
   "metadata": {
    "id": "c8oIM0e409Yi"
   },
   "outputs": [],
   "source": [
    "# Create an id to label name mapping\n",
    "behaviors_id_to_classname = {}\n",
    "for i, v in enumerate(behaviors):\n",
    "    behaviors_id_to_classname[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cBKINsdf1XAo",
   "metadata": {
    "id": "cBKINsdf1XAo"
   },
   "outputs": [],
   "source": [
    "behaviors_id_to_classname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VfyHXTevpWk3",
   "metadata": {
    "id": "VfyHXTevpWk3"
   },
   "outputs": [],
   "source": [
    "# Get the number of classes in your fine-tuning dataset\n",
    "num_classes = len(behaviors)\n",
    "\n",
    "# Modify the final classification layer\n",
    "# The correct attribute name is likely 'projection' within the 'head' module\n",
    "model.blocks[-1].proj = nn.Linear(model.blocks[-1].proj.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a0753",
   "metadata": {
    "id": "041a0753"
   },
   "source": [
    "#### Define input transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uTXrADXdeHw4",
   "metadata": {
    "id": "uTXrADXdeHw4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.hub import load\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4LHRp47ccS2W",
   "metadata": {
    "id": "4LHRp47ccS2W"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle SlowFast input.\"\"\"\n",
    "    batch = [data for data in batch if data is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "\n",
    "    # Separate the inputs and labels\n",
    "    inputs_batch = [item[0] for item in batch]  # List of [slow_tensor, fast_tensor]\n",
    "    labels_batch = torch.stack([item[1] for item in batch])\n",
    "\n",
    "    # Stack the slow and fast pathways separately\n",
    "    slow_pathway_batch = torch.stack([item[0] for item in inputs_batch])\n",
    "    fast_pathway_batch = torch.stack([item[1] for item in inputs_batch])\n",
    "\n",
    "    inputs = [slow_pathway_batch, fast_pathway_batch]\n",
    "\n",
    "    return inputs, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260eb786",
   "metadata": {
    "id": "260eb786"
   },
   "outputs": [],
   "source": [
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0,\n",
    "                frames.shape[1] - 1,\n",
    "                frames.shape[1] // 4,  # slowfast_alpha is usually 4\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "\n",
    "class SlowFastDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, annotations_file, transform=None, clip_duration=None, class_names=None\n",
    "    ):\n",
    "        self.annotations = [json.loads(line) for line in open(annotations_file, \"r\")]\n",
    "        self.transform = transform\n",
    "        self.clip_duration = clip_duration\n",
    "        self.class_names = class_names  # Add class_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        video_path_slow = annotation[\"videos\"][0]\n",
    "        video_path_fast = annotation[\"videos\"][1]\n",
    "        behavior_label = annotation[\"response\"]\n",
    "\n",
    "        label = (\n",
    "            self.class_names.index(behavior_label)\n",
    "            if self.class_names\n",
    "            else behavior_label\n",
    "        )\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        try:\n",
    "            video_slow = EncodedVideo.from_path(video_path_slow)\n",
    "            video_fast = EncodedVideo.from_path(video_path_fast)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading video: {e}\")\n",
    "            return None  # Skip this item\n",
    "\n",
    "        start_sec = 0\n",
    "        end_sec = self.clip_duration\n",
    "\n",
    "        clip_slow_data = video_slow.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "        clip_fast_data = video_fast.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "        if clip_slow_data is None or clip_fast_data is None:\n",
    "            print(f\"Error extracting clips: {video_path_slow} or {video_path_fast}\")\n",
    "            return None  # Skip this item\n",
    "\n",
    "        if self.transform:\n",
    "            try:\n",
    "                frames_slow = self.transform[\"video_slow\"](clip_slow_data[\"video\"])\n",
    "                frames_fast = self.transform[\"video_fast\"](clip_fast_data[\"video\"])\n",
    "            except Exception as e:\n",
    "                print(f\"Error in transforms: {e}\")\n",
    "                return None  # Skip this item\n",
    "\n",
    "            return [frames_slow, frames_fast], label\n",
    "        else:\n",
    "            return [clip_slow_data[\"video\"], clip_fast_data[\"video\"]], label\n",
    "\n",
    "\n",
    "# Define your transforms\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "clip_duration = (num_frames * sampling_rate) / frames_per_second\n",
    "\n",
    "transform_slow = Compose(\n",
    "    [\n",
    "        UniformTemporalSubsample(num_frames // 4),\n",
    "        Lambda(lambda x: x / 255.0),\n",
    "        NormalizeVideo(mean, std),\n",
    "        ShortSideScale(size=side_size),\n",
    "        CenterCropVideo(crop_size=(crop_size, crop_size)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_fast = Compose(\n",
    "    [\n",
    "        UniformTemporalSubsample(num_frames),\n",
    "        Lambda(lambda x: x / 255.0),\n",
    "        NormalizeVideo(mean, std),\n",
    "        ShortSideScale(size=side_size),\n",
    "        CenterCropVideo(crop_size=(crop_size, crop_size)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_transform = {\"video_slow\": transform_slow, \"video_fast\": transform_fast}\n",
    "val_transform = {\"video_slow\": transform_slow, \"video_fast\": transform_fast}\n",
    "\n",
    "# Assuming you have a list of your class names\n",
    "class_names = behaviors\n",
    "\n",
    "# Instantiate your datasets\n",
    "train_dataset = SlowFastDataset(\n",
    "    annotations_file=\"train_video_annotations.jsonl\",\n",
    "    transform=train_transform,\n",
    "    clip_duration=clip_duration,\n",
    "    class_names=class_names,  # Pass class names to the dataset\n",
    ")\n",
    "\n",
    "val_dataset = SlowFastDataset(\n",
    "    annotations_file=\"test_video_annotations.jsonl\",\n",
    "    transform=val_transform,\n",
    "    clip_duration=clip_duration,\n",
    "    class_names=class_names,  # Pass class names here as well\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X7i2smHIulUa",
   "metadata": {
    "id": "X7i2smHIulUa"
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qmQSsNcXtdUn",
   "metadata": {
    "id": "qmQSsNcXtdUn"
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if inputs is None:  # Handle cases where collate_fn returns None\n",
    "            continue\n",
    "\n",
    "        # Move the list of input tensors to the device\n",
    "        inputs = [inp.to(device) for inp in inputs]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}\"\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = [inp.to(device) for inp in inputs]\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {100 * correct / total:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54iXvF1Q2M3w",
   "metadata": {
    "id": "54iXvF1Q2M3w"
   },
   "outputs": [],
   "source": [
    "print(\"Finished Training\")\n",
    "torch.save(model.state_dict(), \"fine_tuned_slowfast_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DP8-sHZKzNJ9",
   "metadata": {
    "id": "DP8-sHZKzNJ9"
   },
   "outputs": [],
   "source": [
    "transform = ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(size=side_size),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate) / frames_per_second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1a4b6",
   "metadata": {
    "id": "16b1a4b6"
   },
   "source": [
    "#### Run Inference\n",
    "\n",
    "Download an example video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8666b5f",
   "metadata": {
    "id": "b8666b5f"
   },
   "outputs": [],
   "source": [
    "video_path = \"my_example_long_video.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7d0a6",
   "metadata": {
    "id": "53d7d0a6"
   },
   "source": [
    "Load the video and transform it to the input format required by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc7f72",
   "metadata": {
    "id": "50bc7f72"
   },
   "outputs": [],
   "source": [
    "# Select the duration of the clip to load by specifying the start and end duration\n",
    "# The start_sec should correspond to where the action occurs in the video\n",
    "start_sec = 0\n",
    "end_sec = start_sec + clip_duration\n",
    "\n",
    "# Initialize an EncodedVideo helper class and load the video\n",
    "video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "# Load the desired clip\n",
    "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "# Apply a transform to normalize the video input\n",
    "video_data = transform(video_data)\n",
    "\n",
    "# Move the inputs to the desired device\n",
    "inputs = video_data[\"video\"]\n",
    "inputs = [i.to(device)[None, ...] for i in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071647f",
   "metadata": {
    "id": "f071647f"
   },
   "source": [
    "#### Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44666862",
   "metadata": {
    "id": "44666862"
   },
   "outputs": [],
   "source": [
    "# Pass the input clip through the model\n",
    "preds = model(inputs)\n",
    "\n",
    "# Get the predicted classes\n",
    "post_act = torch.nn.Softmax(dim=1)\n",
    "preds = post_act(preds)\n",
    "pred_classes = preds.topk(k=5).indices[0]\n",
    "\n",
    "# Map the predicted classes to the label names\n",
    "pred_class_names = [behaviors_id_to_classname[int(i)] for i in pred_classes]\n",
    "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a991646",
   "metadata": {
    "id": "4a991646"
   },
   "source": [
    "### Model Description\n",
    "SlowFast model architectures are based on [1] with pretrained weights using the 8x8 setting\n",
    "on the Kinetics dataset.\n",
    "\n",
    "| arch | depth | frame length x sample rate | top 1 | top 5 | Flops (G) | Params (M) |\n",
    "| --------------- | ----------- | ----------- | ----------- | ----------- | ----------- |  ----------- | ----------- |\n",
    "| SlowFast | R50   | 8x8                        | 76.94 | 92.69 | 65.71     | 34.57      |\n",
    "| SlowFast | R101  | 8x8                        | 77.90 | 93.27 | 127.20    | 62.83      |\n",
    "\n",
    "\n",
    "### References\n",
    "[1] Christoph Feichtenhofer et al, \"SlowFast Networks for Video Recognition\"\n",
    "https://arxiv.org/pdf/1812.03982.pdf"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "annolid-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
