{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KruxdFHpCWGk"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/healthonrails/annolid/blob/main/docs/tutorials/YOLO_SAHI_inference_for_ultralytics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fW_Fs8O0CWGl"
   },
   "source": [
    "## 0. Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwGC2CsiCWGm"
   },
   "source": [
    "- Install latest version of SAHI and ultralytics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lauam-B1CWGm"
   },
   "outputs": [],
   "source": [
    "!pip install -U torch sahi ultralytics\n",
    "!pip install ipywidgets\n",
    "!pip install supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFmY-qvUCWGo"
   },
   "source": [
    "## 1. Sliced Inference with an Ultralytics Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bU0MBuZxCWGo"
   },
   "source": [
    "- Instantiate a detection model by defining model weight path and other parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zMPhYcaGzDH"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "from google.colab.patches import cv2_imshow\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Initialize the detection model (using Ultralytics YOLO weights)\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type=\"ultralytics\",\n",
    "    model_path=\"yolo11n.pt\",  # Use the appropriate model weight file\n",
    "    confidence_threshold=0.35,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "\n",
    "def process_video(\n",
    "    input_video_path,\n",
    "    output_video_path,\n",
    "    slice_height=256,\n",
    "    slice_width=256,\n",
    "    overlap_height_ratio=0.2,\n",
    "    overlap_width_ratio=0.2,\n",
    "    export_dir=\"/content/sahi_video_frames\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a video frame-by-frame by running sliced prediction on each frame.\n",
    "    Annotated frames are generated via SAHI's export_visuals method (which saves a PNG file).\n",
    "    The PNG is then read back as a NumPy array for writing to the output video.\n",
    "    Every 30 frames the annotated frame is displayed inline (suitable for Colab).\n",
    "\n",
    "    Args:\n",
    "        input_video_path (str): Path to the input video file.\n",
    "        output_video_path (str): Path to save the annotated output video.\n",
    "        slice_height (int): Height of each slice used during inference.\n",
    "        slice_width (int): Width of each slice used during inference.\n",
    "        overlap_height_ratio (float): Overlap ratio for slice height.\n",
    "        overlap_width_ratio (float): Overlap ratio for slice width.\n",
    "        export_dir (str): Directory to save temporary annotated frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "\n",
    "    # Create the export directory if it doesn't exist.\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    # Retrieve video properties.\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Set up the video writer for the output video.\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Run sliced prediction on the current frame.\n",
    "        # get_sliced_prediction returns a PredictionResult instance.\n",
    "        result = get_sliced_prediction(\n",
    "            frame,\n",
    "            detection_model,\n",
    "            slice_height=slice_height,\n",
    "            slice_width=slice_width,\n",
    "            overlap_height_ratio=overlap_height_ratio,\n",
    "            overlap_width_ratio=overlap_width_ratio,\n",
    "        )\n",
    "\n",
    "        # Use SAHI's export_visuals to annotate the frame.\n",
    "        # A unique file name is generated for each frame.\n",
    "        file_name = f\"frame_{frame_count}\"\n",
    "        result.export_visuals(\n",
    "            export_dir=export_dir,\n",
    "            file_name=file_name,\n",
    "            text_size=1,\n",
    "            rect_th=2,\n",
    "            hide_conf=False,\n",
    "        )\n",
    "\n",
    "        # Read the annotated frame from disk.\n",
    "        annotated_frame_path = os.path.join(export_dir, f\"{file_name}.png\")\n",
    "        annotated_frame = cv2.imread(annotated_frame_path)\n",
    "\n",
    "        # Write the annotated frame to the output video.\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # Every 30 frames, display the annotated frame inline.\n",
    "        if frame_count % 30 == 0:\n",
    "            cv2_imshow(annotated_frame)\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Processed {frame_count} frames...\")\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(\"Video processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzIBjAEiHF4W"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "input_video = \"/content/video-4.mp4\"\n",
    "output_video = \"/video-4_tracked.mp4\"\n",
    "process_video(input_video, output_video)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
